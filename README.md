# 评论数据挖掘浅探

### 背景

从文本数据中发现有效信息，为产品的更新迭代提供依据。 

### 目的

发现商品评论中对商品的一些负面情感，挖掘商品的一些不足，建议等。 

## python库的介绍与安装

1.       pandas：python的基础库，用以处理原始数据，数据清洗等工作。安装方法：pip install pandas

2.       jieba：主要用来分词以及词性标注：安装方法：pip install jieba

3.       re：用来匹配出需要的字符。安装方法：pip install re

4.       snownlp：对评论的情感进行判断。安装方法：pip install snownlp

5.       gensim：内含包括word2vec在内的多种主题模型算法，相似度计算等，主要用来删除一模板评论。安装方法：pip install gensim

6.       stanfordcorenlp：包含分词，词性标注，句法分析等功能。安装方法：pip install stanfordcorenlp 安装依赖：

①   下载安装JDK 1.8及以上版本。 

②   下载Stanford CoreNLP文件，解压。 

③   处理中文还需要下载中文的模型jar文件，然后放到stanford-corenlp-full-2018-02-27根目录下即可（注意一定要下载这个文件，否则它默认是按英文来处理的）。

7.       Nltk: 包含分词，词性标注，句法分析等功能。

安装方法：

①   Pip install nltk

②   Import nltk

③   nltk.download()这时会自动弹出一nltk数据包下载窗口，点击download 下载.

8. Synonyms：可以用于自然语言理解的很多任务：文本对齐，推荐算法，相似度计算，语义偏移，关键字提取，概念提取，自动摘要，搜索引擎等。安装方法：pip install Synonyms

   ## 文本挖掘涉及的一些算法

   1.       TF-IDF：即Term Frequency-Inverse Document Frequency, 词频-逆文件频率，其原理为一个词语在一篇文章中出现次数越多，同时在所有文档中出现次数越少，就越能代表该文章

   2.       TEXTRANK：textrank算法是一种用于文本的基于图的排序算法。其基本思想来源于谷歌的 PageRank算法, 通过把文本分割成若干组成单元(单词、句子)并建立图模型, 利用投票机制对文本中的重要成分进行排序, 仅利用单篇文档本身的信息即可实现关键词提取、文摘。

   3.       LDA：LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。其效果与PCA相似，都是追求组内方差最小，组件方差最大的原则，与PCA的不同之处是LDA为有监督学习，并且LDA不止可以用来降维，还能用来分类。

   4.       朴素贝叶斯：通过计算词汇在训练集中好评类别出现的频率以及差评类别中出现的频率来判断该评论的情感倾向。

   5.       KDD：其步骤为：

   ①   词性标注

   ②   用寻找频繁项集的方法发现特征词

   ③   剪枝：此算法进行了两次剪枝：

   a.       频繁特征集包含特征短语。由于抽取特征短语不是人工进行的，而是直接把相邻的两个或多个名词组成短语，这样组成的短语不一定有意义。因此需要判断并决定是否剪枝。判断规则是：短语包含的两个或多个名词如果出现在同一个句子中，则按顺序计算两两之间的距离。如果每两个名词之间都不超过3个单词的距离，则认为这个特征短语是compact的（有影响力的）。如果特征短语有影响力的句子不超过2个，则认为这个特征短语应该被去除掉，即剪枝。

   b.       对于特征A，如果某个特征短语也包含A，那么计算A在句子中单独出现的次数（包含A的特征短语不能出现在这个句子中）。如果次数小于一个阈值，那么该特征短语被去除。

   ④   判断句子情感：

   a.       预设30个极性明显的形容词，存储在seedList，寻找观点词同义词和反义词，若词汇不存在于seedlist，则添加，直至seedlist的大小较难扩充为止。

   b.       抽取句子中的所有形容词，判断其词性，包含一个Positive词性的形容词就加1，包含一个Negative词性的形容词就减1。如果句子最后的值大于0，则句子的情感为Positive；如果值小于0，则为Negative。如果刚好等于0，那么就认为当前句子的情感方向与前一个句子的情感方向是一样的。

   6.       Word2vec：主要将词汇向量化，使词汇能够像数字一样计算距离（如余弦相似度、欧式距离等），来判断它们之间的语义相似度。3

   7. LSTM：结合循环神经网络的NLP深度学习方法，实现难度较大。

      ### 曾经碰到的一些问题

      ①   Word2vec训练模型时最好能有大于5GB的样本集进行训练，否则结果较差，或者用别人已经训练好的模型。

      ②   TF-IDF、LDA大多数是用来提取文章的关键词，常用来聚类，分类等，短文本的效果较差。 

      ③   Textrank虽然对于短文本来说效果要比TF-IDF好，但是其最终结果还是不满意，主要有两点：1.提取的关键词准确性较差2.牺牲了句子的可读性。

      ④   Nltk是专门用来处理文本数据的，里面的功能也较为丰富，大多数的文本处理方法都有涵盖，但是就具体性能而言，它的分词性能要逊色于jieba，实体识别、句法依存关于、句法结构树等性能也比不上stanfordcorenlp，因此不使用。

      ⑤   Snownlp模块的改动：

      a.       将sentiment文件夹中int.py文件中的seg函数（分词函数）替换为jieba的函数

      b.       将sentiment中加载模型的代码删掉，从外部用sentiment.load()调用，因为经过测试，那段代码为无效代码。

      ⑥   使用jieba.load_userdict()调用自定义库，自定义库的格式为一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。

      ⑦   贝叶斯模型采用的方法是通过计算评论中的词在训练样本中好评类别里出现的频率和在差评类别中出现的频率，从而判断该评论的情感倾向。该方法较为简单，对一些复杂句无判断能力，例如双重否定句。解决方法：可以尝试引进复杂的模型（如LSTM）解决。

      ⑧   Jieba中的词性标注模块虽然也有分词的效果，但其实际的分词结果与cut模块可能会不一样，具体原因尚不清楚。

      ⑨   Jieba中也有调整词频的方法，但是都是一次性的东西，当清空缓存后就要重新读，因此制作词库是比较一劳永逸的方法。

      ## 评论挖掘的步骤

      1.       评论情感判断

      a.       调用discover_feature.py中的sen(data_list)函数其中data_list为一个评论列表，其原理是调用snownip中的sentiment函数对每条评论进行情感判断，在最后的结果中。情感系数大于等于0.4的记为好评，小于0.4的记为差评，最后输出，sentiment的核心算法是朴素贝叶斯原理。

      b.       建立正向词库以及负向词库，计算评论中正向词汇与负向词汇的占比，若正向词汇多于负向词汇，则判断该评论为正向，反之则判断为负向，若一样多，则根据关键词特征进行正向或者负向的判断，若评论中无情感词汇，则采用a方法判断。

      2.       关键词提取

      调用discover_feature.py中的discover_feature(data_list,*text)函数，其中data_list为评论列表，*text为关键词，算法核心思想为将评论拆分成小句，若小句中出现关键词，则匹配关键词之后的所有字段，并进行词性标注，取出距离关键词最近的修饰词，组合成短语输出。

      3.       根据关键词分类

      调用discover_feature.py中的keys_classification(data_list,*text)函数，其中data_list为评论列表，*text为关键词，主要思想为若评论中出现了关键词，则将评论取出。

      ##初探句子主干提取

主干提取主要是调用StanfordCoreNLP来分析句子的句法结构，然后根据句子的句法结构制定规则来提取句子主干，本文只是做了初步的尝试，可以对一些较为简单的句子提取主干。